\section{Methods}
\label{sec:methods}
\subsection*{Overview}
My proposed method involves first modifying the Efros-Leung and PatchMatch algorithms to use multiple source images to fill a hole in a target image. The results from these two algorithms will be compared to output from the LaMa neural network, which will only be given the target image, as it does not have the capability to accept sources. A peak signal-to-noise ratio cannot be calculated for hole filling, as no ground truth exists, but I will calculate PSNR for the target image generation of the basic and multiple reference image PatchMatch algorithms using the following:
$$MSE = \frac1{3n}||\mathbf{T} - \mathbf{G}||_F^2$$
$$20\ln255 - 10\ln(MSE)$$

Where $n$ is the number of pixels, $\mathbf{T}$ is the matrix of the output target image, and $\mathbf{G}$ is the matrix of the ground truth image.

\subsection*{Comparison to state-of-the-art}
Although the PatchMatch paper was published in 2009, based on my research it is the most recently created handcrafted algorithm for hole filling. In the original paper, when hole filling, only a single image can be provided, and that single image is split into a target and source. Compared to the original PatchMatch algorithm, my modified version has the benefit of being able to use multiple source images for hole filling. 

The state-of-the-art neural network I chose to compare to is LaMa, published in 2021. Though it seems LaMa does outperform PatchMatch in terms of plausibility and runtime, my modified PatchMatch does have the advantage of not requiring training time or training data, being able to accept multiple source images, and being able to ``peek" around the hole by looking for similar patches in source images.

\subsection*{Efros-Leung}
The modification of the Efros-Leung algorithm was quite simple. I first started with the Efros-Leung texture synthesis code written in Homework 2 and removed the initialization code. Then, I changed the code to accept inputs with RGBA color channels, where the alpha channel is used to denote holes. Finally, I changed the code to iterate over all pixels in all reference images, rather than only a single reference image.

\subsection*{PatchMatch}
Writing the modified PatchMatch algorithm was a bit more involved. For this project, I chose to write it in Python because I wanted to choose a language that I could be sure groupmates taking this class would know. I tried to start off by writing an algorithm which would do hole filling on multiple reference images, but I found that the code became unorganized and difficult to understand. Instead, I decided to first begin with the basic PatchMatch algorithm of creating a target image using patches of a single source image.

In my code, I first initialized a random NNF and calculated a SSD for each vector in the NNF, which I called the nearest neighbor distance (NND). Then, I wrote the propagation step. Here, I achieved the scanline and reverse scanline order by simply reversing the order of a list of indices each iteration. However, I would have preferred to write my own doubly linked list data structure if the language had not been Python. During the propagation step, I would check either the vectors of the left and top neighbors or the bottom and right neighbors of the pixel and compare the SSD of that vector against the pixel's NND. If the new SSD was lower than the NND, I would update both the pixel's NNF and the pixel's NND with the new values from the neighboring pixel. After the propagation step and before moving on to the next pixel, I would do a random search over an exponentially decreasing area. The way I implemented this was to first generate a random search vector: 
$$\vec{R} = \langle R_x, R_y \rangle$$
$$R_x \in [\max(-s, -V_x), \min(s, h-V_x-1)]$$
$$R_y \in [\max(-s, -V_y), \min(s, w-V_y-1)]$$

Where $s$ is the current maximum length of the radius of the search area, $\vec V$ is the current NNF vector, and $h,w$ are the dimensions of the image. Then, the SSD of the generated vector is compared to the NND, and like in the propagation step, if the match is better the NNF and NND are updated. The radius length $r$ begins at the size of the larger dimension of the image $\max(h,w)$. After each random search, $r$ is halved via integer division to exponentially decrease the search area. This is why the PatchMatch algorithm has $O(n\log n)$ complexity.

After five iterations, the algorithm is stopped, because at this point it is highly likely to have converged to or close to the best NNF. At this point, the algorithm fills in an empty image with pixels selected using vectors from the NNF.

The reason why the random initialization works and is able to produce a coherent output similar to the target is due to the fact that clusters of nearby pixels or patches within a simillar target and source will tend to be spatially related. Suppose we have $n$ pixels in an image, and a specific patch contains $p$ pixels. If only a single pixel in the target patch is randomly assigned to the correct source patch, then that NNF vector will be propagated to all its neighbors. Mathematically,
$$P(\text{best match found}) \ge 1 - \left(\frac {n-p}n\right)^{(t+1)p}$$

Where $t$ is the amount of iterations. This is a valid lower bound because each iteration adds a random search across the image. We see that even if the target is a $1000\times1000$ pixel image and the patch is only a $30\times30$ patch, the probability of the best match being found is over $99.27\%$. This is not even considering the fact that matches other than the best match may be found, and that each random search performs $\log n$ searches. So the probability of an acceptable match being found nears $100\%$.

Once I was finished implementing the basic version of the PatchMatch algorithm, I modified the code to use multiple reference images. The main change between this version and the previous version was an introduction of a third dimension to the NNF vectors. In addition to an $x$ and $y$ component describing a horizontal and vertical translation from target to source, the NNF vectors now included a third component corresponding to a source image. In the case of a video, we can think of this third dimension as a time dimension. In addition to this change, I also added a new step between propagation and random search, where for each reference image, I chose a random patch and compared distances. Like the random search step, this step was meant to prevent getting stuck in local minima.

After this, I modified the program once again to fill holes rather than recreate a target image. In order to do this, I first partitioned the target image into two subimages, one consisting of everything within a bounding box around the hole, and another consisting of everything outside of that bounding box. To determine the size of the bounding box, I took the minimal radius of the hole $r$, and extended the minimal bounding box by that radius. Then, the previous PatchMatch algorithm is executed on the nontransparent pixels within the target to generate a NNF.

With the NNF generated, for each transparent pixel in the target, the algorithm does an accumulation of NNF vector votes from nontransparent pixels whose location in the target image is within $r$ from the location of the transparent pixel. After the accumulation of votes, each vector is applied to the transparent pixel location to generate a color from the source images, and a weighted average is applied to these generated colors and placed in the final output.

Finally, I wrote an algorithm that would first downscale the target and source image dimensions by a factor of $f^3$, and then apply the hole filling algorithm before iteratively upscaling the hole's dimensions by a factor of $f$, downscaling the target and source images to match in scale, and applying the second algorithm, this time with the entire filled target image as the target and the references without the target as the source. In this case I found $f = 1.3$ to have good results. The purpose of this is to reduce the runtime of vote accumulation, which has a complexity of $O(r^2)$, and also to mitigate blurriness in the output due to conflicting NNF vector votes.